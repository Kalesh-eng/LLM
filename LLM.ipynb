{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLRBvxAIR45w7LHBN2L5uv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalesh-eng/LLM/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. is this a test a test.\"\n",
        "result=re.split(r'(\\s)', text)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRvTXmaqEgxJ",
        "outputId": "8ed428e2-aab0-459b-d9a8-e26aeb5d29db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'is', ' ', 'this', ' ', 'a', ' ', 'test', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. is This-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?\"`()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rybc88xcIpdp",
        "outputId": "c115c4b3-5c0a-4cda-b821-9438790d789f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'is', 'This', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "with open('/the-verdict.txt', encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# print(\"Total number of character:\", len(raw_text))\n",
        "\n",
        "# print(raw_text[:90])  # Print the first 90 characters\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?\"`()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KesgI39YxjQy",
        "outputId": "40fdb58b-94bc-4144-c02e-c472b0f6e29f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "print(f\"{len(all_words)} unique words found\")\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrs08XJU0V4_",
        "outputId": "cab3ac23-50ff-4c5f-fbe1-a6ea8142dd30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1157 unique words found\n",
            "1157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqnJcIJw1Ckz",
        "outputId": "532b048b-5008-4e6a-9a1b-a9f9323ab6e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, \"'\": 1, '(': 2, ')': 3, ',': 4, '--': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'A': 10, 'Ah': 11, 'Among': 12, 'And': 13, 'Are': 14, 'Arrt': 15, 'As': 16, 'At': 17, 'Be': 18, 'Begin': 19, 'Burlington': 20, 'But': 21, 'By': 22, 'Carlo': 23, 'Chicago': 24, 'Claude': 25, 'Come': 26, 'Croft': 27, 'Destroyed': 28, 'Devonshire': 29, 'Don': 30, 'Dubarry_': 31, 'Emperors': 32, 'Florence': 33, 'For': 34, 'Gallery': 35, 'Gideon': 36, 'Gisburn': 37, 'Gisburn!': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jack!': 58, 'Jove': 59, 'Jove!': 60, 'Just': 61, 'Lord': 62, 'Made': 63, 'Miss': 64, 'Money': 65, 'Monte': 66, 'Moon-dancers': 67, 'Mr': 68, 'Mrs': 69, 'My': 70, 'Never': 71, 'No': 72, 'Now': 73, 'Nutley': 74, 'Of': 75, 'Oh': 76, 'On': 77, 'Once': 78, 'Only': 79, 'Or': 80, 'Perhaps': 81, 'Poor': 82, 'Professional': 83, 'Renaissance': 84, 'Rickham': 85, 'Rickham!': 86, 'Riviera': 87, 'Rome': 88, 'Russian': 89, 'Sevres': 90, 'She': 91, 'Stroud': 92, 'Stroud!': 93, 'Strouds': 94, 'Suddenly': 95, 'That': 96, 'The': 97, 'Then': 98, 'There': 99, 'They': 100, 'This': 101, 'Those': 102, 'Though': 103, 'Thwing': 104, 'Thwings': 105, 'To': 106, 'Usually': 107, 'Venetian': 108, 'Victor': 109, 'Was': 110, 'We': 111, 'Well': 112, 'Well!': 113, 'What': 114, 'When': 115, 'Why': 116, 'Yes': 117, 'You': 118, '_I': 119, '_am_': 120, '_famille-verte_': 121, '_felt_': 122, '_has_': 123, '_have_': 124, '_jardiniere_': 125, '_mine_': 126, '_not_': 127, '_rose': 128, '_rs_': 129, '_that_': 130, '_the_': 131, '_was_': 132, '_were_': 133, 'a': 134, 'abdication': 135, 'able': 136, 'about': 137, 'above': 138, 'abruptly': 139, 'absolute': 140, 'absorbed': 141, 'absurdity': 142, 'academic': 143, 'accuse': 144, 'accustomed': 145, 'across': 146, 'activity': 147, 'add': 148, 'added': 149, 'admirers': 150, 'adopted': 151, 'adulation': 152, 'advance': 153, 'aesthetic': 154, 'affect': 155, 'afraid': 156, 'after': 157, 'afterward': 158, 'again': 159, 'ago': 160, 'ah': 161, 'air': 162, 'alive': 163, 'all': 164, 'almost': 165, 'alone': 166, 'along': 167, 'always': 168, 'amazement': 169, 'amid': 170, 'among': 171, 'amplest': 172, 'amusing': 173, 'an': 174, 'and': 175, 'another': 176, 'answer': 177, 'answered': 178, 'any': 179, 'anything': 180, 'anywhere': 181, 'apparent': 182, 'apparently': 183, 'appearance': 184, 'appeared': 185, 'appointed': 186, 'are': 187, 'arm': 188, 'arm-chair': 189, 'arm-chairs': 190, 'arms': 191, 'art': 192, 'articles': 193, 'artist': 194, 'as': 195, 'aside': 196, 'asked': 197, 'at': 198, 'atmosphere': 199, 'atom': 200, 'attack': 201, 'attention': 202, 'attitude': 203, 'audacities': 204, 'away': 205, 'awful': 206, 'axioms': 207, 'azaleas': 208, 'back': 209, 'background': 210, 'balance': 211, 'balancing': 212, 'balustraded': 213, 'basking': 214, 'bath-rooms': 215, 'be': 216, 'beaming': 217, 'bean-stalk': 218, 'bear': 219, 'beard': 220, 'beauty': 221, 'became': 222, 'because': 223, 'becoming': 224, 'bed': 225, 'been': 226, 'before': 227, 'began': 228, 'begun': 229, 'behind': 230, 'being': 231, 'believed': 232, 'beneath': 233, 'bespoke': 234, 'better': 235, 'between': 236, 'big': 237, 'bits': 238, 'bitterness': 239, 'blocked': 240, 'born': 241, 'borne': 242, 'boudoir': 243, 'bravura': 244, 'break': 245, 'breaking': 246, 'breathing': 247, 'bric-a-brac': 248, 'briefly': 249, 'brings': 250, 'bronzes': 251, 'brought': 252, 'brown': 253, 'brush': 254, 'bull': 255, 'business': 256, 'but': 257, 'buying': 258, 'by': 259, 'called': 260, 'came': 261, 'can': 262, 'canvas': 263, 'canvases': 264, 'cards': 265, 'care': 266, 'career': 267, 'caught': 268, 'central': 269, 'chair': 270, 'chap': 271, 'characteristic': 272, 'charming': 273, 'cheap': 274, 'check': 275, 'cheeks': 276, 'chest': 277, 'chimney-piece': 278, 'chucked': 279, 'cigar': 280, 'cigarette': 281, 'cigars': 282, 'circulation': 283, 'circumstance': 284, 'circus-clown': 285, 'claimed': 286, 'clasping': 287, 'clear': 288, 'cleverer': 289, 'close': 290, 'clue': 291, 'coat': 292, 'collapsed': 293, 'colour': 294, 'come': 295, 'comfortable': 296, 'coming': 297, 'companion': 298, 'compared': 299, 'complex': 300, 'confident': 301, 'congesting': 302, 'conjugal': 303, 'constraint': 304, 'consummate': 305, 'contended': 306, 'continued': 307, 'corner': 308, 'corrected': 309, 'could': 310, 'couldn': 311, 'count': 312, 'countenance': 313, 'couple': 314, 'course': 315, 'covered': 316, 'craft': 317, 'cried': 318, 'crossed': 319, 'crowned': 320, 'crumbled': 321, 'cry': 322, 'cured': 323, 'curiosity': 324, 'curious': 325, 'current': 326, 'curtains': 327, 'd': 328, 'dabble': 329, 'damask': 330, 'dark': 331, 'dashed': 332, 'day': 333, 'days': 334, 'dead': 335, 'deadening': 336, 'dear': 337, 'deep': 338, 'deerhound': 339, 'degree': 340, 'delicate': 341, 'demand': 342, 'denied': 343, 'deploring': 344, 'deprecating': 345, 'deprecatingly': 346, 'desire': 347, 'destroyed': 348, 'destruction': 349, 'desultory': 350, 'detail': 351, 'diagnosis': 352, 'did': 353, 'didn': 354, 'died': 355, 'dim': 356, 'dimmest': 357, 'dingy': 358, 'dining-room': 359, 'disarming': 360, 'discovery': 361, 'discrimination': 362, 'discussion': 363, 'disdain': 364, 'disdained': 365, 'disease': 366, 'disguised': 367, 'display': 368, 'dissatisfied': 369, 'distinguished': 370, 'distract': 371, 'divert': 372, 'do': 373, 'doesn': 374, 'doing': 375, 'domestic': 376, 'don': 377, 'done': 378, 'donkey': 379, 'down': 380, 'dozen': 381, 'dragged': 382, 'drawing-room': 383, 'drawing-rooms': 384, 'drawn': 385, 'dress-closets': 386, 'drew': 387, 'dropped': 388, 'each': 389, 'earth': 390, 'ease': 391, 'easel': 392, 'easy': 393, 'echoed': 394, 'economy': 395, 'effect': 396, 'effects': 397, 'efforts': 398, 'egregious': 399, 'eighteenth-century': 400, 'elbow': 401, 'elegant': 402, 'else': 403, 'embarrassed': 404, 'enabled': 405, 'end': 406, 'endless': 407, 'enjoy': 408, 'enlightenment': 409, 'enough': 410, 'ensuing': 411, 'equally': 412, 'equanimity': 413, 'escape': 414, 'established': 415, 'etching': 416, 'even': 417, 'event': 418, 'ever': 419, 'everlasting': 420, 'every': 421, 'exasperated': 422, 'except': 423, 'excuse': 424, 'excusing': 425, 'existed': 426, 'expected': 427, 'exquisite': 428, 'exquisitely': 429, 'extenuation': 430, 'exterminating': 431, 'extracting': 432, 'eye': 433, 'eyebrows': 434, 'eyes': 435, 'face': 436, 'faces': 437, 'fact': 438, 'faded': 439, 'failed': 440, 'failure': 441, 'fair': 442, 'faith': 443, 'false': 444, 'familiar': 445, 'fancy': 446, 'fashionable': 447, 'fate': 448, 'feather': 449, 'feet': 450, 'fell': 451, 'fellow': 452, 'felt': 453, 'few': 454, 'fewer': 455, 'finality': 456, 'find': 457, 'fingers': 458, 'first': 459, 'fit': 460, 'fitting': 461, 'five': 462, 'flash': 463, 'flashed': 464, 'florid': 465, 'flowers': 466, 'fluently': 467, 'flung': 468, 'follow': 469, 'followed': 470, 'fond': 471, 'footstep': 472, 'for': 473, 'forced': 474, 'forcing': 475, 'forehead': 476, 'foreign': 477, 'foreseen': 478, 'forgive': 479, 'forgotten': 480, 'form': 481, 'formed': 482, 'forming': 483, 'forward': 484, 'fostered': 485, 'found': 486, 'foundations': 487, 'fragment': 488, 'fragments': 489, 'frame': 490, 'frames': 491, 'frequently': 492, 'friend': 493, 'from': 494, 'full': 495, 'fullest': 496, 'furiously': 497, 'furrowed': 498, 'garlanded': 499, 'garlands': 500, 'gave': 501, 'genial': 502, 'genius': 503, 'gesture': 504, 'get': 505, 'getting': 506, 'give': 507, 'given': 508, 'glad': 509, 'glanced': 510, 'glimpse': 511, 'gloried': 512, 'glory': 513, 'go': 514, 'going': 515, 'gone': 516, 'good': 517, 'good-breeding': 518, 'good-humoured': 519, 'got': 520, 'grace': 521, 'gradually': 522, 'gray': 523, 'grayish': 524, 'great': 525, 'greatest': 526, 'greatness': 527, 'grew': 528, 'groping': 529, 'growing': 530, 'had': 531, 'hadn': 532, 'hair': 533, 'half': 534, 'half-light': 535, 'half-mechanically': 536, 'hall': 537, 'hand': 538, 'hands': 539, 'handsome': 540, 'hanging': 541, 'happen': 542, 'happened': 543, 'hard': 544, 'hardly': 545, 'have': 546, 'haven': 547, 'having': 548, 'he': 549, 'head': 550, 'hear': 551, 'heard': 552, 'heart': 553, 'height': 554, 'her': 555, 'here': 556, 'hermit': 557, 'herself': 558, 'hesitations': 559, 'hide': 560, 'high': 561, 'him': 562, 'him!': 563, 'himself': 564, 'hint': 565, 'his': 566, 'his!': 567, 'history': 568, 'holding': 569, 'home': 570, 'honour': 571, 'hooded': 572, 'hostess': 573, 'hot-house': 574, 'hour': 575, 'hours': 576, 'house': 577, 'how': 578, 'hung': 579, 'husband': 580, 'idea': 581, 'idle': 582, 'idling': 583, 'if': 584, 'immediately': 585, 'in': 586, 'incense': 587, 'indifferent': 588, 'inevitable': 589, 'inevitably': 590, 'inflexible': 591, 'insensible': 592, 'insignificant': 593, 'instinctively': 594, 'instructive': 595, 'interesting': 596, 'into': 597, 'ironic': 598, 'irony': 599, 'irrelevance': 600, 'irrevocable': 601, 'is': 602, 'it': 603, 'its': 604, 'itself': 605, 'jealousy': 606, 'just': 607, 'keep': 608, 'kept': 609, 'kind': 610, 'knees': 611, 'knew': 612, 'know': 613, 'known_': 614, 'laid': 615, 'lair': 616, 'landing': 617, 'language': 618, 'last': 619, 'late': 620, 'later': 621, 'latter': 622, 'laugh': 623, 'laughed': 624, 'lay': 625, 'leading': 626, 'lean': 627, 'learned': 628, 'least': 629, 'leathery': 630, 'leave': 631, 'led': 632, 'left': 633, 'leisure!': 634, 'lends': 635, 'lent': 636, 'let': 637, 'lies!': 638, 'life': 639, 'life-likeness': 640, 'lift': 641, 'lifted': 642, 'light': 643, 'lightly': 644, 'like': 645, 'liked': 646, 'line': 647, 'lines': 648, 'lingered': 649, 'lips': 650, 'lit': 651, 'little': 652, 'live': 653, 'll': 654, 'loathing': 655, 'long': 656, 'longed': 657, 'longer': 658, 'look': 659, 'looked': 660, 'looking': 661, 'lose': 662, 'loss': 663, 'lounging': 664, 'lovely': 665, 'lucky': 666, 'lump': 667, 'luncheon-table': 668, 'luxury': 669, 'lying': 670, 'made': 671, 'make': 672, 'man': 673, 'manage': 674, 'managed': 675, 'mantel-piece': 676, 'marble': 677, 'married': 678, 'may': 679, 'me': 680, 'me!': 681, 'meant': 682, 'mediocrity': 683, 'medium': 684, 'mentioned': 685, 'mere': 686, 'merely': 687, 'met': 688, 'might': 689, 'mighty': 690, 'millionaire': 691, 'mine': 692, 'minute': 693, 'minutes': 694, 'mirrors': 695, 'modest': 696, 'modesty': 697, 'moment': 698, 'money': 699, 'monumental': 700, 'mood': 701, 'morbidly': 702, 'more': 703, 'most': 704, 'mourn': 705, 'mourned': 706, 'moustache': 707, 'moved': 708, 'much': 709, 'muddling': 710, 'multiplied': 711, 'murmur': 712, 'muscles': 713, 'must': 714, 'my': 715, 'myself': 716, 'mysterious': 717, 'naive': 718, 'near': 719, 'nearly': 720, 'negatived': 721, 'nervous': 722, 'nervousness': 723, 'neutral': 724, 'never': 725, 'next': 726, 'no': 727, 'none': 728, 'not': 729, 'note': 730, 'note!': 731, 'nothing': 732, 'now': 733, 'nymphs': 734, 'oak': 735, 'obituary': 736, 'object': 737, 'objects': 738, 'occurred': 739, 'oddly': 740, 'of': 741, 'off': 742, 'often': 743, 'oh': 744, 'old': 745, 'on': 746, 'once': 747, 'one': 748, 'ones': 749, 'only': 750, 'onto': 751, 'open': 752, 'or': 753, 'other': 754, 'our': 755, 'ourselves': 756, 'out': 757, 'outline': 758, 'oval': 759, 'over': 760, 'over!': 761, 'own': 762, 'packed': 763, 'paid': 764, 'paint': 765, 'painted': 766, 'painter': 767, 'painting': 768, 'pale': 769, 'paled': 770, 'palm-trees': 771, 'panel': 772, 'panelling': 773, 'pardonable': 774, 'pardoned': 775, 'part': 776, 'passages': 777, 'passing': 778, 'past': 779, 'past!': 780, 'pastels': 781, 'pathos': 782, 'patient': 783, 'people': 784, 'perceptible': 785, 'perfect': 786, 'persistence': 787, 'persuasively': 788, 'phrase': 789, 'picture': 790, 'pictures': 791, 'pines': 792, 'pink': 793, 'place': 794, 'placed': 795, 'plain': 796, 'platitudes': 797, 'pleased': 798, 'pockets': 799, 'point': 800, 'poised': 801, 'poor': 802, 'portrait': 803, 'posing': 804, 'possessed': 805, 'poverty': 806, 'predicted': 807, 'preliminary': 808, 'presenting': 809, 'prestidigitation': 810, 'pretty': 811, 'previous': 812, 'price': 813, 'pride': 814, 'princely': 815, 'prism': 816, 'problem': 817, 'proclaiming': 818, 'prodigious': 819, 'profusion': 820, 'protest': 821, 'prove': 822, 'public': 823, 'purblind': 824, 'purely': 825, 'pushed': 826, 'put': 827, 'qualities': 828, 'quality': 829, 'queerly': 830, 'question': 831, 'quickly': 832, 'quietly': 833, 'quite': 834, 'quote': 835, 'rain': 836, 'raised': 837, 'random': 838, 'rather': 839, 're': 840, 'real': 841, 'really': 842, 'reared': 843, 'reason': 844, 'reassurance': 845, 'recovering': 846, 'recreated': 847, 'reflected': 848, 'reflection': 849, 'regrets': 850, 'relatively': 851, 'remained': 852, 'remember': 853, 'reminded': 854, 'repeating': 855, 'represented': 856, 'reproduction': 857, 'resented': 858, 'resolve': 859, 'resources': 860, 'rest': 861, 'rich': 862, 'ridiculous': 863, 'robbed': 864, 'romantic!': 865, 'room': 866, 'rose': 867, 'rule': 868, 'run': 869, 's': 870, 'said': 871, 'same': 872, 'satisfaction': 873, 'savour': 874, 'saw': 875, 'say': 876, 'saying': 877, 'says': 878, 'scorn': 879, 'scornful': 880, 'secret': 881, 'see': 882, 'seemed': 883, 'seen': 884, 'self-confident': 885, 'send': 886, 'sensation': 887, 'sensitive': 888, 'sent': 889, 'serious': 890, 'set': 891, 'sex': 892, 'shade': 893, 'shaking': 894, 'shall': 895, 'she': 896, 'shirked': 897, 'short': 898, 'should': 899, 'shoulder': 900, 'shoulders': 901, 'show': 902, 'showed': 903, 'showy': 904, 'showy!': 905, 'shrug': 906, 'shrugged': 907, 'sight': 908, 'sign': 909, 'silent': 910, 'silver': 911, 'similar': 912, 'simpleton': 913, 'simplifications': 914, 'simply': 915, 'since': 916, 'single': 917, 'sitter': 918, 'sitters': 919, 'sketch': 920, 'skill': 921, 'slight': 922, 'slightly': 923, 'slowly': 924, 'small': 925, 'smile': 926, 'smiling': 927, 'sneer': 928, 'so': 929, 'solace': 930, 'some': 931, 'somebody': 932, 'something': 933, 'spacious': 934, 'spaniel': 935, 'speaking-tubes': 936, 'speculations': 937, 'spite': 938, 'splash': 939, 'square': 940, 'stairs': 941, 'stammer': 942, 'stand': 943, 'standing': 944, 'started': 945, 'stay': 946, 'stay!': 947, 'still': 948, 'stocked': 949, 'stood': 950, 'stopped': 951, 'stopping': 952, 'straddling': 953, 'straight': 954, 'strain': 955, 'straining': 956, 'strange': 957, 'straw': 958, 'stream': 959, 'stroke': 960, 'strokes': 961, 'strolled': 962, 'strongest': 963, 'strongly': 964, 'struck': 965, 'studio': 966, 'stuff': 967, 'subject': 968, 'substantial': 969, 'suburban': 970, 'such': 971, 'suddenly': 972, 'suffered': 973, 'sugar': 974, 'suggested': 975, 'sunburn': 976, 'sunburnt': 977, 'sunlit': 978, 'superb': 979, 'sure': 980, 'surest': 981, 'surface': 982, 'surprise': 983, 'surprised': 984, 'surrounded': 985, 'suspected': 986, 'sweetly': 987, 'sweetness': 988, 'swelling': 989, 'swept': 990, 'swum': 991, 't': 992, 'table': 993, 'take': 994, 'taken': 995, 'talking': 996, 'tea': 997, 'tears': 998, 'technicalities': 999, 'technique': 1000, 'tell': 1001, 'tells': 1002, 'tempting': 1003, 'terra-cotta': 1004, 'terrace': 1005, 'terraces': 1006, 'terribly': 1007, 'than': 1008, 'that': 1009, 'the': 1010, 'their': 1011, 'them': 1012, 'then': 1013, 'there': 1014, 'therefore': 1015, 'they': 1016, 'thin': 1017, 'thing': 1018, 'things': 1019, 'think': 1020, 'this': 1021, 'thither': 1022, 'those': 1023, 'though': 1024, 'thought': 1025, 'three': 1026, 'threshold': 1027, 'threw': 1028, 'through': 1029, 'throwing': 1030, 'tie': 1031, 'till': 1032, 'time': 1033, 'timorously': 1034, 'tinge': 1035, 'tips': 1036, 'tired': 1037, 'to': 1038, 'told': 1039, 'tone': 1040, 'tones': 1041, 'too': 1042, 'took': 1043, 'tottering': 1044, 'touched': 1045, 'toward': 1046, 'trace': 1047, 'trade': 1048, 'transmute': 1049, 'traps': 1050, 'travelled': 1051, 'tribute': 1052, 'tributes': 1053, 'tricks': 1054, 'tried': 1055, 'trouser-presses': 1056, 'true': 1057, 'truth': 1058, 'turned': 1059, 'twenty': 1060, 'twenty-four': 1061, 'twice': 1062, 'twirling': 1063, 'unaccountable': 1064, 'uncertain': 1065, 'under': 1066, 'underlay': 1067, 'underneath': 1068, 'understand': 1069, 'unexpected': 1070, 'untouched': 1071, 'unusual': 1072, 'up': 1073, 'up-stream': 1074, 'upon': 1075, 'upset': 1076, 'upstairs': 1077, 'us': 1078, 'us!': 1079, 'used': 1080, 'usual': 1081, 'value': 1082, 'varnishing': 1083, 'vases': 1084, 've': 1085, 'veins': 1086, 'velveteen': 1087, 'very': 1088, 'villa': 1089, 'vindicated': 1090, 'virtuosity': 1091, 'vista': 1092, 'vocation': 1093, 'voice': 1094, 'wall': 1095, 'wander': 1096, 'want': 1097, 'wanted': 1098, 'wants': 1099, 'was': 1100, 'wasn': 1101, 'watched': 1102, 'watching': 1103, 'water-colour': 1104, 'waves': 1105, 'way': 1106, 'weekly': 1107, 'weeks': 1108, 'welcome': 1109, 'went': 1110, 'were': 1111, 'what': 1112, 'when': 1113, 'whenever': 1114, 'where': 1115, 'which': 1116, 'while': 1117, 'white': 1118, 'white-panelled': 1119, 'who': 1120, 'whole': 1121, 'whom': 1122, 'why': 1123, 'wide': 1124, 'widow': 1125, 'wife': 1126, 'wife!': 1127, 'wild': 1128, 'wincing': 1129, 'window-curtains': 1130, 'wish': 1131, 'with': 1132, 'without': 1133, 'wits': 1134, 'woman': 1135, 'woman!': 1136, 'women': 1137, 'won': 1138, 'wonder': 1139, 'wonder!': 1140, 'wondered': 1141, 'word': 1142, 'work': 1143, 'work!': 1144, 'working': 1145, 'worth': 1146, 'would': 1147, 'wouldn': 1148, 'year': 1149, 'years': 1150, 'yellow': 1151, 'yet': 1152, 'you': 1153, 'younger': 1154, 'your': 1155, 'yourself': 1156}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## correct LLM Encoding and Decoding script ##############################\n",
        "import re\n",
        "\n",
        "# 1. Load the text file (adjust path if needed)\n",
        "with open('/the-verdict.txt', encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# 2. Tokenize the text into words (and punctuation), then make a vocab\n",
        "tokens = re.split(r'([,.:;?\"`()\\']|--|\\s)', raw_text)\n",
        "tokens = [t.strip() for t in tokens if t.strip()]\n",
        "\n",
        "# 3. Make sure \"<unk>\" is in the token list\n",
        "if \"<unk>\" not in tokens:\n",
        "    tokens.append(\"<unk>\")\n",
        "\n",
        "# 4. Make vocab: word -> index\n",
        "all_words = list(set(tokens))\n",
        "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
        "\n",
        "# 5. Tokenizer class\n",
        "class simpletokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i: s for s, i in self.str_to_int.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.split(r'([,.:;?\"`()\\']|--|\\s)', text)\n",
        "        tokens = [item.strip() for item in tokens if item.strip()]\n",
        "        tokens = [\n",
        "            item if item in self.str_to_int else \"<unk>\"\n",
        "            for item in tokens\n",
        "        ]\n",
        "        ids = [self.str_to_int[token] for token in tokens]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        tokens = [self.int_to_str[id] for id in ids]\n",
        "        text = \" \".join(tokens)\n",
        "        text = re.sub(r'\\s+([,.:;?\"`()\\'])', r'\\1', text)  # fix spacing before punctuation\n",
        "        return text\n",
        "\n",
        "# 6. Create tokenizer\n",
        "tokenizer = simpletokenizer(vocab)\n",
        "\n",
        "#7. adding endoftext\n",
        "text1 =  \"hello, do you like tea?\"\n",
        "Text2 =  \"do you like cricket\"\n",
        "text = \"<|endoftext|>\".join([text1, Text2])\n",
        "print (text)\n",
        "\n",
        "# 8. Sample text to test\n",
        "text = \"hello GPT, I am unknownword.\"\n",
        "\n",
        "# 9. Encode & decode\n",
        "encoded = tokenizer.encode(text)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "# 10. Output\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Encoded IDs:\", encoded)\n",
        "print(\"Decoded Text:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W7YT-Zk1jsP",
        "outputId": "8a7b117a-c11a-4c4a-96d6-59cceac8d109"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello, do you like tea?<|endoftext|>do you like cricket\n",
            "Original Text: hello GPT, I am unknownword.\n",
            "Encoded IDs: [557, 557, 1088, 952, 557, 557, 150]\n",
            "Decoded Text: <unk> <unk>, I <unk> <unk>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Byte Pair Encoder  session 8\n",
        "\n",
        "!pip3 install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNDh32mT5LEz",
        "outputId": "a88732f1-e72b-4160-f190-abdb42e0536b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(tiktoken.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMS7jt2A9gc5",
        "outputId": "ba115a54-9d1b-465a-d7e9-ef0c07d2eb8f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.name)\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sulit terraces\"\n",
        "    \"of someunknownPlace.\")\n",
        "\n",
        "integers =tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtSS2B0d-dwH",
        "outputId": "4680d9d6-f8c8-4ae4-c985-f826b63b0cfc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt2\n",
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 33154, 270, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcxa7UZQ4Ljj",
        "outputId": "d2cb451c-56d0-48e6-cf57-051c6ced921a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sulit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "yTby6KCL5-nJ"
      }
    }
  ]
}